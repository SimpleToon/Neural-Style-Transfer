{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2648d115-1453-4bfe-aeec-2a4903014db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "from rembg import remove\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "from LPIPS import LPIPS\n",
    "from ImageProcessor import ImageProcessor\n",
    "\n",
    "from NST import NST\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a57be0-330b-4e05-b1f6-32e441d40074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on https://github.com/ceshine/fast-neural-style/\n",
    "class JohnsonModel(NST):\n",
    "    def __init__(self, content_path, style_paths, prebuild_encoder = None, prebuild_decoder = None):\n",
    "        super().__init__(content_path, style_paths, prebuild_encoder, prebuild_decoder)\n",
    "        self.norm_mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n",
    "        self.norm_std  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n",
    "        self.transformer = self.TransformerNet()\n",
    "        \n",
    "    def uploadModel(self, input):\n",
    "        self.encoder = input\n",
    "        self.loss_net = self.LossNetwork(self.encoder)\n",
    "\n",
    "    def loadModel(self, path):\n",
    "        eState = torch.load(path, map_location=\"cpu\")\n",
    "        self.transformer.load_state_dict(eState)\n",
    "       \n",
    "    class LossNetwork(nn.Module):\n",
    "        def __init__(self, encoder):\n",
    "            super().__init__()\n",
    "            self.vgg_layers = encoder\n",
    "            self.layer_name_mapping = {\n",
    "                '3': \"relu1\",\n",
    "                '8': \"relu2\",\n",
    "                '17': \"relu3\",\n",
    "                '26': \"relu4\",\n",
    "                '35': \"relu5\",\n",
    "            }\n",
    "    \n",
    "        def forward(self, x):\n",
    "            output = {}\n",
    "            for name, module in self.vgg_layers._modules.items():\n",
    "                x = module(x)\n",
    "                if name in self.layer_name_mapping:\n",
    "                    output[self.layer_name_mapping[name]] = x\n",
    "            return output\n",
    "            \n",
    "    class TransformerNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.ConvLayer = JohnsonModel.ConvLayer\n",
    "            self.ResidualBlock = JohnsonModel.ResidualBlock\n",
    "            self.UpsampleConvLayer = JohnsonModel.UpsampleConvLayer\n",
    "            # Initial convolution layers\n",
    "            self.conv1 = self.ConvLayer(3, 32, kernel_size=9, stride=1)\n",
    "            self.in1 = nn.InstanceNorm2d(32, affine=True)\n",
    "            self.conv2 = self.ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "            self.in2 = nn.InstanceNorm2d(64, affine=True)\n",
    "            self.conv3 = self.ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "            self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
    "            # Residual layers\n",
    "            self.res1 = self.ResidualBlock(128)\n",
    "            self.res2 = self.ResidualBlock(128)\n",
    "            self.res3 = self.ResidualBlock(128)\n",
    "            self.res4 = self.ResidualBlock(128)\n",
    "            self.res5 = self.ResidualBlock(128)\n",
    "            # Upsampling Layers\n",
    "            self.deconv1 = self.UpsampleConvLayer(\n",
    "                128, 64, kernel_size=3, stride=1, upsample=2)\n",
    "            self.in4 = nn.InstanceNorm2d(64, affine=True)\n",
    "            self.deconv2 = self.UpsampleConvLayer(\n",
    "                64, 32, kernel_size=3, stride=1, upsample=2)\n",
    "            self.in5 = nn.InstanceNorm2d(32, affine=True)\n",
    "            self.deconv3 = self.ConvLayer(32, 3, kernel_size=9, stride=1)\n",
    "            # Non-linearities\n",
    "            self.relu = nn.ReLU()\n",
    "    \n",
    "        def forward(self, X):\n",
    "            y = self.relu(self.in1(self.conv1(X)))\n",
    "            y = self.relu(self.in2(self.conv2(y)))\n",
    "            y = self.relu(self.in3(self.conv3(y)))\n",
    "            y = self.res1(y)\n",
    "            y = self.res2(y)\n",
    "            y = self.res3(y)\n",
    "            y = self.res4(y)\n",
    "            y = self.res5(y)\n",
    "            y = self.relu(self.in4(self.deconv1(y)))\n",
    "            y = self.relu(self.in5(self.deconv2(y)))\n",
    "            y = self.deconv3(y)\n",
    "            return y\n",
    "\n",
    "\n",
    "    class ConvLayer(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "            super().__init__()\n",
    "            reflection_padding = kernel_size // 2\n",
    "            self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "            self.conv2d = torch.nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "        def forward(self, x):\n",
    "            out = self.reflection_pad(x)\n",
    "            out = self.conv2d(out)\n",
    "            return out\n",
    "    \n",
    "    \n",
    "    class ResidualBlock(nn.Module):\n",
    "    \n",
    "        def __init__(self, channels):\n",
    "            super().__init__()\n",
    "            self.ConvLayer = JohnsonModel.ConvLayer\n",
    "            self.conv1 = self.ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "            self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "            self.conv2 = self.ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "            self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "            self.relu = nn.ReLU()\n",
    "    \n",
    "        def forward(self, x):\n",
    "            residual = x\n",
    "            out = self.relu(self.in1(self.conv1(x)))\n",
    "            out = self.in2(self.conv2(out))\n",
    "            out = out + residual\n",
    "            return out\n",
    "    \n",
    "    \n",
    "    class UpsampleConvLayer(nn.Module):\n",
    "\n",
    "        def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "            super().__init__()\n",
    "            self.upsample = upsample\n",
    "            if upsample:\n",
    "                self.upsample_layer = torch.nn.Upsample(\n",
    "                    mode='nearest', scale_factor=upsample)\n",
    "            reflection_padding = kernel_size // 2\n",
    "            self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "            self.conv2d = torch.nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "        def forward(self, x):\n",
    "            x_in = x\n",
    "            if self.upsample:\n",
    "                x_in = self.upsample_layer(x_in)\n",
    "            out = self.reflection_pad(x_in)\n",
    "            out = self.conv2d(out)\n",
    "            return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def gram_matrix(y):\n",
    "        (b, ch, h, w) = y.size()\n",
    "        features = y.view(b, ch, w * h)\n",
    "        features_t = features.transpose(1, 2)\n",
    "        gram = features.bmm(features_t) / (ch * h * w)\n",
    "        return gram\n",
    "\n",
    "    def normalisation(self, input):\n",
    "        return (input - self.norm_mean) / self.norm_std\n",
    "\n",
    "    @staticmethod\n",
    "    def tv_loss(img):\n",
    "        return torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:])) + \\\n",
    "               torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]))\n",
    "        \n",
    "    def prepare_style(self):\n",
    "        style = self.styles[0].tensorisedImage(1) \n",
    "        \n",
    "        feats_s = self.loss_net(style)\n",
    "\n",
    "        self._style_grams = {k: self.gram_matrix(v) for k, v in feats_s.items()}\n",
    "\n",
    "    def train(self, train_loader, steps=300, lr=1e-3,content_weight=1.0, style_weight=1e5, regularisation=1e-6, log_interval=50):\n",
    "\n",
    "        if self._style_grams is None:\n",
    "            self.prepare_style()\n",
    "\n",
    "        self.transformer.train()\n",
    "        optim = torch.optim.Adam(self.transformer.parameters(), lr=lr)\n",
    "        mse = nn.MSELoss()\n",
    "\n",
    "        step = 0\n",
    "        running_c = running_s = running_tv = 0.0\n",
    "\n",
    "        while step < steps:\n",
    "            for x, _ in train_loader:\n",
    "                step += 1\n",
    "                if step > steps:\n",
    "                    break\n",
    "\n",
    "                optim.zero_grad()\n",
    "\n",
    "                y = self.transformer(x)\n",
    "\n",
    "                y_n = self.normalisation(y)\n",
    "                x_n = self.normalisation(x)\n",
    "                \n",
    "                feats_y = self.loss_net(y_n)\n",
    "                feats_x = self.loss_net(x_n)\n",
    "\n",
    "                content_loss = content_weight * mse(feats_y[\"relu2\"], feats_x[\"relu2\"].detach())\n",
    "\n",
    "                style_loss = 0.0\n",
    "                for k in self._style_grams.keys():\n",
    "                    style_loss = style_loss + mse(self.gram_matrix(feats_y[k]), self._style_grams[k].expand_as(self.gram_matrix(feats_y[k])))\n",
    "                style_loss = style_weight * style_loss\n",
    "\n",
    "                tv = regularisation * self.tv_loss(y)\n",
    "\n",
    "                total = content_loss + style_loss + tv\n",
    "                total.backward()\n",
    "                optim.step()\n",
    "\n",
    "                running_c += float(content_loss)\n",
    "                running_s += float(style_loss)\n",
    "                running_tv += float(tv)\n",
    "\n",
    "                if step % log_interval == 0:\n",
    "                    print(f\"[{step}/{steps}] content={running_c/log_interval:.2f} style={running_s/log_interval:.2f} tv={running_tv/log_interval:.6f}\")\n",
    "                    running_c = running_s = running_tv = 0.0\n",
    "\n",
    "    def pipeline(self, train_loader, output_path):\n",
    "        self.prepare_style()\n",
    "        #Start counter (as new image need training)\n",
    "        self.start = time.perf_counter()\n",
    "        \n",
    "        self.train(\n",
    "        train_loader=train_loader,\n",
    "        steps = 2000)\n",
    "        torch.save(self.transformer.state_dict(), output_path)\n",
    "        \n",
    "\n",
    "    def stylise(self):\n",
    "        self.stylisingTime = time.perf_counter()\n",
    "        content = self.content.tensorisedImage(1) \n",
    "        output = self.transformer(content)\n",
    "        self.end = time.perf_counter() #end timer\n",
    "        \n",
    "        self.stylisedTensor = output.detach().clamp(0, 1)\n",
    "        self.stylisedImage = self.stylisedTensor.squeeze(0).permute(1,2,0).numpy()\n",
    "\n",
    "        timePassed = (self.end - self.stylisingTime) * 1000\n",
    "\n",
    "        print(f\"Style Time:{timePassed:.3f} ms\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48845da6-2724-4f1d-81cf-64d023065dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.eval()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root= \"coco\",             \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "image_path = \"content/avril.jpg\"\n",
    "style_path = \"style/sketch.png\"\n",
    "\n",
    "nstModel = JohnsonModel(image_path,[style_path])\n",
    "nstModel.uploadModel(model)\n",
    "nstModel.processor()\n",
    "nstModel.pipeline(train_loader, \"johnson-sketch.pth\")\n",
    "nstModel.stylise()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4cad0-f412-4fc6-af3e-57a4eeaea004",
   "metadata": {},
   "outputs": [],
   "source": [
    "nstModel.displayImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6d51c-2891-4e35-9cb7-8cf19ef783d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nstModel.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18b03c-c3b6-4a55-a1e8-1131b3ca1fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
